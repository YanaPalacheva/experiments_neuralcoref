{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def rucor_to_neuralcoref(path_conllu, path_out):\n",
    "    if os.path.exists(path_out):\n",
    "            shutil.rmtree(path_out)\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    \n",
    "    # iterate over files in dir\n",
    "    for doc in os.listdir(path_conllu):\n",
    "        print(doc)\n",
    "        # read the file as string\n",
    "        with open(os.path.join(path_conllu, doc), 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        # parse Rucor conllu from string\n",
    "        # coreference data is written in the \"misc\" col and divided by \"|\"\n",
    "        split_func = lambda line, i: line[i].split(\"|\")\n",
    "        sentences = parse(data, \n",
    "                          fields=[\"id\", \"form\", \"lemma\", \"upos\", \"xpos\", \"feats\", \"head\", \"deprel\", \"deps\", \"misc\"],  \n",
    "                          field_parsers={\"misc\": split_func})\n",
    "        \n",
    "        dirt = []\n",
    "        # extract group_id and chain_id from the col 'misc' and add them as cols\n",
    "        for sentence in sentences:\n",
    "            # remove unnessesary metadata\n",
    "            sentence.metadata = {}\n",
    "            for token in sentence:\n",
    "                # the corpus contains non-sentence data; we'll remove it later   \n",
    "                if not isinstance(token['id'], int):\n",
    "                    dirt.append(sentence)\n",
    "                    break\n",
    "                    \n",
    "                # remember id (it should start from 0 instead of 1) and form\n",
    "                t_id = token['id'] - 1\n",
    "                t_form = token['form']\n",
    "                \n",
    "                # add doc name and part num to the beginning; shift id and form to the next cols\n",
    "                token['id'] = path_out.split(os.path.sep)[-1][-3:] + '/' + doc[:-6]\n",
    "                token['form'] = 0\n",
    "                token['lemma'] = t_id\n",
    "                token['upos'] = t_form\n",
    "                \n",
    "                # read coref data and empty the col\n",
    "                # neuralcoref looks for Speaker data in the 9th col (\"misc\"); the corpus does not contain this information\n",
    "                attrs = token['misc']\n",
    "                token['misc'] = None\n",
    "\n",
    "                # extract group_id from coref data and add it to conllu\n",
    "                matching = [s for s in attrs if \"RuCor_group_id\" in s]\n",
    "                if matching:\n",
    "                    token['group_id'] = matching[0].split('=')[1]\n",
    "                    if len(matching) > 1:\n",
    "                        print(len(matching))\n",
    "                else:\n",
    "                    token['group_id'] = '_'\n",
    "\n",
    "                # extract chain_id from coref data and add it to conllu\n",
    "                matching = [s for s in attrs if \"RuCor_chain_id\" in s]\n",
    "                if matching:\n",
    "                    token['chain_id'] = matching[0].split('=')[1]\n",
    "                    if len(matching) > 1:\n",
    "                        print(len(matching))\n",
    "                else:\n",
    "                    token['chain_id'] = '_'\n",
    "                \n",
    "                # neuralcoref looks for coreference chains in the last col; it accepts '-' for empty values\n",
    "                token['coref'] = '-'\n",
    "        \n",
    "        # remove dirty data\n",
    "        if dirt:\n",
    "            for d in dirt:\n",
    "                print(' '.join(str(t['form']) for t in d))\n",
    "            sentences = [a for a in sentences if a not in dirt]\n",
    "        \n",
    "        # add coreference chains to token[\"coref\"]\n",
    "        # neuralcoref expects chains like \"(chain_id)\",\n",
    "        # where \"(\" and \")\" represent the beginning and the end of a unique coreference group described by group_id\n",
    "        end = {}\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for j, token in enumerate(sentence):\n",
    "\n",
    "                if token['chain_id'] != '_':\n",
    "                    # new part of the group\n",
    "                    if token['group_id'] in end:\n",
    "                        token['coref'] = token['chain_id'] + ')'\n",
    "                        # remove ')' from the previous token of the group\n",
    "                        i_prev, j_prev =  end[token['group_id']]\n",
    "                        sentences[i_prev][j_prev]['coref'] = sentences[i_prev][j_prev]['coref'][:-1]\n",
    "                    # first occurence\n",
    "                    else:\n",
    "                        token['coref'] = '(' + token['chain_id'] + ')'\n",
    "\n",
    "                    # remember coordinates of the last seen group_id\n",
    "                    end[token['group_id']] = (i, j)\n",
    "        \n",
    "        # write to .conll file\n",
    "        with open(os.path.join(path_out, doc[:-6]+'.v4_gold_conll'), 'a', encoding='utf-8') as f:\n",
    "            f.write('#begin document (' + path_out.split(os.path.sep)[-1][-3:] + '/' + doc[:-6] + '); part 000\\n')\n",
    "            for sentence in sentences:\n",
    "                f.write(sentence.serialize())\n",
    "            f.write('#end document ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102_beliajev_nad_bezdnoj.conll\n",
      "107_dragunsky_volshebnaja_sila_iskusstva.conll\n",
      "15_paustovsky_zhilcy_starogo_doma.conll\n",
      "2_astafiev_zhizn_prozhit.conll\n",
      "30_dojl_sluchaj.conll\n",
      "34_kassil_solnce_svetit.conll\n",
      "43_musatov_stozhary.conll\n",
      "44_nagibin_siren.conll\n",
      "53_beliajev_dom_s_prividenijami.conll\n",
      "5_petrushevskaya_v_detstve.conll\n",
      "67_zamiatin_kolumb.conll\n",
      "73_ilf_schastlivy_otec.conll\n",
      "andersen_motylek.conll\n",
      "bazhov_travyanaja_zapadenka.conll\n",
      "bunin_skazka.conll\n",
      "dostojevskij_podrostok.conll\n",
      "dovlatov_kompromiss_6.conll\n",
      "fet_knyaginya.conll\n",
      "gilyarovskij_moi_skitanija.conll\n",
      "gogol_zapiski_3.conll\n",
      "harms_upadanije.conll\n",
      "_ сентября _\n",
      "korolenko_mgnovenije.conll\n",
      "strugackije_ponedelnik.conll\n",
      "turgenev_veshnije_vody.conll\n",
      "2013_04_11_dotless_.conll\n",
      "2013_07_31_krebs_.conll\n",
      "lenta.ru-news-2014-01-19-cutshort.conll\n",
      "lenta.ru-news-2014-01-24-if.conll\n",
      "lenta.ru-news-2014-01-30-crimea.conll\n",
      "lenta.ru-news-2014-02-03-capitanic.conll\n",
      "lenta.ru-news-2014-02-03-london.conll\n",
      "lenta.ru-news-2014-02-03-name1.conll\n",
      "lenta.ru-news-2014-02-03-rucksack.conll\n",
      "lenta.ru-news-2014-02-04-party.conll\n",
      "lenta.ru-news-2014-02-04-pyramid.conll\n",
      "lentaru001.conll\n",
      "lentaru002.conll\n",
      "lentaru003.conll\n",
      "lentaru004.conll\n",
      "lentaru005.conll\n",
      "lentaru006.conll\n",
      "lentaru007.conll\n",
      "lentaru008.conll\n",
      "lentaru009.conll\n",
      "lentaru010.conll\n",
      "lentaru011.conll\n",
      "lentaru012.conll\n",
      "lentaru013.conll\n",
      "lentaru014.conll\n",
      "lentaru015.conll\n",
      "lentaru016.conll\n",
      "lentaru017.conll\n",
      "lentaru018.conll\n",
      "lentaru019.conll\n",
      "lentaru020.conll\n",
      "boldyrewae.conll\n",
      "ermolaew2.conll\n",
      "filippow.conll\n",
      "kolobkowa.conll\n",
      "kowalewskij.conll\n",
      "lyalin2.conll\n",
      "maslov.conll\n",
      "ufo.conll\n",
      "ugrumowa.conll\n",
      "yaglov.conll\n",
      "2009-abbas5_ru.conll\n",
      "2009-abbas6_ru.conll\n",
      "2009-abbas7_ru.conll\n",
      "2009-abusada7_ru.conll\n",
      "2009-ahtisaari3_ru.conll\n",
      "2009-annan2_ru.conll\n",
      "2009-annan3_ru.conll\n",
      "2009-aslund24_ru.conll\n",
      "2009-asteiner3_ru.conll\n",
      "2009-asteiner5_ru.conll\n",
      "2009-avineri35_ru.conll\n",
      "2009-bakker3_ru.conll\n",
      "2009-bakker4_ru.conll\n",
      "2009-baradei1_ru.conll\n",
      "2009-barbarosie1_ru.conll\n",
      "2009-barnett1_ru.conll\n",
      "2009-barroso1_ru.conll\n",
      "2009-barroso3_ru.conll\n",
      "2009-beasley1_ru.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ шести - и семилетних детей из 73 исследовательских центров 31 страны мира .\n",
      "2009-bebchuk1_ru.conll\n",
      "duma_time.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ ) .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 _\n",
      "m_l_king.conll\n",
      "0 0 0 0 0 0 0 _\n",
      "sunna.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 _\n",
      "10.conll\n",
      "100.conll\n",
      "101.conll\n",
      "102.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ cannibale , то есть `` молодые каннибалы `` , или `` юные людоеды `` .\n",
      "103.conll\n",
      "106.conll\n",
      "11.conll\n",
      "0 _ денег АвтоВАЗу не давать .\n",
      "0 _ потребовать от компании план реорганизации бизнеса , направленный на снижение издержек .\n",
      "0 _ 26 млрд рублей выделить потребителям , то есть российским покупателям этих машин , сделав их дешевле .\n",
      "0 _ на следующий год заложить в бюджете 75 % от этой суммы , а на 2011 год - 50 % , чтобы у ВАЗа было время на серьезную перестройку .\n",
      "0 _ разрешить ВАЗу сократить ВСЕХ избыточных работников , которым за счет бюджета в течение трех лет будут выплачиваться специальные субсидии в размере пособия по безработице ( то есть получится двойное пособие , то есть 10 тыс. рублей в месяц ) .\n",
      "0 _ если окажется , что ВАЗ не сможет ( не захочет , не сумеет ) перестроить свой бизнес так , чтобы оказаться прибыльным и устойчивым , то `` Ростехнологии '' должны продать свой 25-процентный пакет другому акционеру , компании Renault , за 1 рубль , а Российская Федерация должна заплатить Renault тот самый миллиард долларов , который Renault заплатил за свой пакет , и разрешить делать с этим заводом всё что угодно .\n",
      "110.conll\n",
      "111.conll\n",
      "112.conll\n",
      "113.conll\n",
      "116.conll\n",
      "118.conll\n",
      "119.conll\n",
      "12.conll\n",
      "120.conll\n",
      "121.conll\n",
      "122.conll\n",
      "123.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ материал .\n",
      "_ `` Shine On '' на самом деле не о Сиде - он стал эдаким символом крайнего отчуждения , которому предаются некоторые люди , потому что это единственный способ для них вынести весь этот чёртов мрак - современную жизнь - и полностью устраниться .\n",
      "124.conll\n",
      "128.conll\n",
      "130.conll\n",
      "132.conll\n",
      "136.conll\n",
      "138.conll\n",
      "139.conll\n",
      "140.conll\n",
      "142.conll\n",
      "145.conll\n",
      "15.conll\n",
      "19.conll\n",
      "2.conll\n",
      "3.conll\n",
      "4.conll\n",
      "5.conll\n",
      "51.conll\n",
      "0 0 0 0 0 0 0 0 0 0 _ 6 , 5 , 4 , _ , _ , _ , _ , _ , _ , _ , _ и _ .\n",
      "0 0 0 0 _ ( 38 000 км ) , максимальный - кольцо _ ( приблизительно 98 000 км ) .\n",
      "0 0 0 0 0 0 0 0 _ _ и _ состоят из мелких частиц пыли , тогда как узкое тусклое _ содержит крупные тела .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ .\n",
      "0 0 0 0 0 0 0 0 0 0 _ 6 , 5 , 4 , _ , _ , _ , _ , _ , _ , _ , _ , и _ кольца .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ , _ , _ , _ , _ , _ ) , два пылевых кольца _ _ ) и два внешних кольца ( _ , _ ) .\n",
      "0 0 0 0 0 0 0 0 _ _ , _ , _ , _ и _ кольцах .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ Урана являются узкими , относительно тёмными и оба `` пасутся '' парой спутников .\n",
      "_ ( эпсилон )\n",
      "0 _ ( эпсилон ) - самое яркое и самое плотное из колец Урана и ответственно примерно за две трети света , отражаемого кольцами .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ было одним из двух колец , чья ширина различима на камерах `` Вояджера '' .\n",
      "0 0 0 0 0 _ - около 47 километров .\n",
      "0 0 0 _ достоверно неизвестна , хотя , по некоторым оценкам , составляет примерно 150 метров .\n",
      "0 _ - достаточно `` переполненное место '' с коэффициентом заполнения оценённым различными источниками от 0 , 008 до 0 , 06 вблизи апоцентра .\n",
      "0 0 0 0 0 _ исчезает при наблюдении с ребра .\n",
      "0 0 0 _ была подтверждена многими последующими наблюдениями покрытий .\n",
      "0 _ выглядит состоящим из множества узких и оптически плотных колечек .\n",
      "0 0 _ оценивается около 1016 кг .\n",
      "52.conll\n",
      "53.conll\n",
      "0 0 0 0 0 0 0 0 0 0 _ ( `` сома '' ) , которое переводится как тело или оболочка , и _ ( `` ерион '' ) - шерсть .\n",
      "54.conll\n",
      "55.conll\n",
      "57.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ alle da '' все ограничения на реализацию .\n",
      "0 0 0 _ alle da '' подвергся цензуре в ноябре 2009 года , когда Немецким Федеральным Комитетом по Оценке СМИ обложка диска была названа пропагандирующей садомазохизм .\n",
      "64.conll\n",
      "65.conll\n",
      "66.conll\n",
      "67.conll\n",
      "68.conll\n",
      "0 0 0 0 _ _ - английский профессиональный футбольный клуб из Стретфорда , Большой Манчестер .\n",
      "69.conll\n",
      "7.conll\n",
      "70.conll\n",
      "71.conll\n",
      "72.conll\n",
      "75.conll\n",
      "77.conll\n",
      "79.conll\n",
      "80.conll\n",
      "82.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ для системы `` Галилео '' .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ и российской навигационной системой ГЛОНАСС .\n",
      "88.conll\n",
      "9.conll\n",
      "90.conll\n",
      "95.conll\n",
      "347-done.conll\n",
      "388-done.conll\n",
      "448-done.conll\n",
      "516-done.conll\n",
      "540-done.conll\n",
      "554-done.conll\n",
      "559-done.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ )\n",
      "675-done.conll\n",
      "682-done.conll\n",
      "689-done.conll\n",
      "789-done.conll\n",
      "833-done.conll\n",
      "842-done.conll\n",
      "850-done.conll\n",
      "870-done.conll\n",
      "890-done.conll\n",
      "895-done.conll\n",
      "908-done.conll\n",
      "921-done.conll\n",
      "992-done.conll\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r59401.conll\n",
      "0 0 0 0 0 0 0 0 0 0 _ / Когда читала про отель , люди писали , что хотели бы вернуться сюда , не понимала , как можно ехать в одно и тоже место дважды ?\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r67576.conll\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r68708.conll\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r69802.conll\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r72202.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ отель , но пройдя по территории , она производит удручающее впечатление , зелени мало , одни корпуса и дорожки между ними , а у Портесов - территории не большие ...\n",
      "PhotoDescr1.conll\n",
      "PhotoDescr11.conll\n",
      "PhotoDescr15.conll\n",
      "09Jan2014_bloomberg_sleep.html.conll\n",
      "nauka i zhizn_mars.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ темные пятна `` морей '' .\n",
      "nauka i zhizn_pererabotka.conll\n",
      "0 0 0 0 0 0 0 0 0 0 _ 95 , жидких порядка 750 .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 _ 200 дм3 .\n",
      "0 0 0 0 0 0 0 0 0 _ 5 , высокого прессования обычно не превышает 10 и зависит от вида ТРО : у металлических отходов ( 8 _ 10 ) , у резинотехнических изделий и полимерных материалов ( 2 _ 3 ) .\n",
      "0 0 0 _ .\n",
      "0 0 0 0 _ все органические соединения , составляющие ТРО , переходят в газообразное состояние и удаляются на систему газоочистки , неорганические - плавятся .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ .\n",
      "0 0 0 0 0 0 0 0 _ 669 , 2 _ , - является продуктом ядерного распада .\n",
      "0 0 0 0 0 0 0 _ 2 , 9 г/см3 .\n",
      "0 0 0 0 0 0 0 0 0 0 0 _ _ 40 тн .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philology.ru-linguistics1-alpatov-12-out2.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ , отсюда миф об их происхождении от смешения людей с собаками .\n",
      "philology.ru-linguistics1-barannikov-46-out2.conll\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ ) , полного ( 1852-1875 ) и краткого ( 1879-1889 ) .\n",
      "0 0 0 0 0 0 0 0 0 0 0 _ , принимали участие не только работники Академии , но и многочисленные ученые разных стран Европы .\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 _ по линии изучения буддизма .\n"
     ]
    }
   ],
   "source": [
    "path_conllu = '.\\\\rucoref_29.10.2015\\\\parsed_testset'\n",
    "path_out = '.\\\\rucoref_29.10.2015\\\\parsed_testset_neuralcoref'\n",
    "\n",
    "for doc in os.listdir(path_conllu):\n",
    "    rucor_to_neuralcoref(os.path.join(path_conllu, doc), os.path.join(path_out, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
