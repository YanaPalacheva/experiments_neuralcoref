{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def paws_to_scorer(path_conllu, path_out, for_cort=False):\n",
    "    if os.path.exists(path_out):\n",
    "        shutil.rmtree(path_out)\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    \n",
    "    # iterate over files in dir\n",
    "    for doc in os.listdir(path_conllu):\n",
    "        if not doc.endswith('.en.conll'):\n",
    "            continue\n",
    "        print(doc)\n",
    "        # read the file as string\n",
    "        with open(os.path.join(path_conllu, doc), 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        # parse all the fields in a given conllu document\n",
    "        sentences = parse(data, \n",
    "                          fields=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "                                  \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\"])\n",
    "        # the last sentence is empty; conllu.parse() expects other formatting\n",
    "        sentences = sentences[:-1]\n",
    "        \n",
    "        # rearrange data to make it suitable for scorer\n",
    "        for sentence in sentences:\n",
    "            # remove unnessesary metadata\n",
    "            sentence.metadata = {}\n",
    "            for i, token in enumerate(sentence):\n",
    "                t_form = token['1']\n",
    "                \n",
    "                # add doc name and part num to the beginning\n",
    "                token['0'] = doc[:-6]\n",
    "                token['1'] = 0\n",
    "                # shift id and form to the next cols \n",
    "                # id is a token number in the sentence, starting with i=0\n",
    "                token['2'] = i\n",
    "                token['3'] = t_form\n",
    "                \n",
    "                if token['4'] == '_':\n",
    "                    token['4'] = '-'\n",
    "                if token['5'] == '_':\n",
    "                    token['5'] = '*'\n",
    "                \n",
    "                if for_cort:\n",
    "                    start = 10\n",
    "                else:\n",
    "                    start = 4\n",
    "                \n",
    "                # remove unnecessary cols\n",
    "                for i in range(start,17):\n",
    "                    del token[str(i)]\n",
    "                \n",
    "                # the scorer accepts '-' for empty values\n",
    "                if token['17'] == '_':\n",
    "                    token['17'] = '-'\n",
    "        \n",
    "        # write to .conll file\n",
    "        with open(os.path.join(path_out, doc[:-6]+'.conll'), 'a', encoding='utf-8') as f:\n",
    "            f.write('#begin document (' + doc[:-6] + '); part 000\\n')\n",
    "            for sentence in sentences:\n",
    "                f.write(sentence.serialize())\n",
    "            f.write('#end document ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1900.en.conll\n",
      "wsj_1901.en.conll\n",
      "wsj_1902.en.conll\n",
      "wsj_1903.en.conll\n",
      "wsj_1904.en.conll\n",
      "wsj_1905.en.conll\n",
      "wsj_1906.en.conll\n",
      "wsj_1907.en.conll\n",
      "wsj_1908.en.conll\n",
      "wsj_1909.en.conll\n",
      "wsj_1910.en.conll\n",
      "wsj_1911.en.conll\n",
      "wsj_1912.en.conll\n",
      "wsj_1913.en.conll\n",
      "wsj_1914.en.conll\n",
      "wsj_1915.en.conll\n",
      "wsj_1916.en.conll\n",
      "wsj_1917.en.conll\n",
      "wsj_1918.en.conll\n",
      "wsj_1919.en.conll\n",
      "wsj_1920.en.conll\n",
      "wsj_1921.en.conll\n",
      "wsj_1922.en.conll\n",
      "wsj_1923.en.conll\n",
      "wsj_1924.en.conll\n",
      "wsj_1925.en.conll\n",
      "wsj_1926.en.conll\n",
      "wsj_1927.en.conll\n",
      "wsj_1928.en.conll\n",
      "wsj_1929.en.conll\n",
      "wsj_1930.en.conll\n",
      "wsj_1931.en.conll\n",
      "wsj_1932.en.conll\n",
      "wsj_1933.en.conll\n",
      "wsj_1934.en.conll\n",
      "wsj_1935.en.conll\n",
      "wsj_1936.en.conll\n",
      "wsj_1937.en.conll\n",
      "wsj_1938.en.conll\n",
      "wsj_1939.en.conll\n",
      "wsj_1940.en.conll\n",
      "wsj_1941.en.conll\n",
      "wsj_1942.en.conll\n",
      "wsj_1943.en.conll\n",
      "wsj_1944.en.conll\n",
      "wsj_1945.en.conll\n",
      "wsj_1946.en.conll\n",
      "wsj_1947.en.conll\n",
      "wsj_1948.en.conll\n",
      "wsj_1949.en.conll\n"
     ]
    }
   ],
   "source": [
    "path_conllu = '.\\\\corpora\\\\paws\\\\data\\\\conll'\n",
    "path_out = '.\\\\scorer\\\\key'\n",
    "\n",
    "paws_to_scorer(path_conllu, path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def ontonotes_to_scorer(path_conllu, path_out, path_plain, for_cort=False):\n",
    "    if os.path.exists(path_out):\n",
    "        shutil.rmtree(path_out)\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    \n",
    "    if not for_cort:\n",
    "        if os.path.exists(path_plain):\n",
    "            shutil.rmtree(path_plain)\n",
    "        os.makedirs(path_plain, exist_ok=True)\n",
    "    \n",
    "    with open(path_conllu, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # parse all the fields in a given conllu document\n",
    "    sentences = parse(data, \n",
    "                      fields=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "                              \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"])\n",
    "    # the last sentence is empty; conllu.parse() expects other formatting\n",
    "    sentences = sentences[:-1]\n",
    "    \n",
    "    # rearrange data to make it suitable for scorer\n",
    "    for sentence in sentences:\n",
    "        for i, token in enumerate(sentence):\n",
    "            \n",
    "            # modify doc name for unification purposes\n",
    "            token['0'] = token['0'].split('/')[-1]\n",
    "            \n",
    "            if for_cort:\n",
    "                start = 11\n",
    "                token['10'] = token[str(max(token, key=int))]\n",
    "                if token['4'] == '_':\n",
    "                    token['4'] = '-'\n",
    "                if token['5'] == '_':\n",
    "                    token['5'] = '*'\n",
    "            else:\n",
    "                start = 5\n",
    "                token['4'] = token[str(max(token, key=int))]\n",
    "\n",
    "            # remove unnecessary cols\n",
    "            for i in range(start,20):\n",
    "                if str(i) in token.keys():\n",
    "                    del token[str(i)]\n",
    "\n",
    "    # write to .conll file\n",
    "    header = ''\n",
    "    part = ''\n",
    "    for sentence in sentences:\n",
    "        if sentence[0]['0'] != header or sentence[0]['1'] != part:\n",
    "            if header != '':\n",
    "                with open(os.path.join(path_out, header.split('/')[-1] + '_' + part + '.en.conll'), 'a', encoding='utf-8') as f:\n",
    "                    f.write('#end document\\n')\n",
    "            header = sentence[0]['0']\n",
    "            part = sentence[0]['1']\n",
    "            with open(os.path.join(path_out, header.split('/')[-1] + '_' + part + '.en.conll'), 'a', encoding='utf-8') as f:\n",
    "                f.write('#begin document (' + header + '_' + part + '); part 000\\n')\n",
    "        with open(os.path.join(path_out, header.split('/')[-1] + '_' + part + '.en.conll'), 'a', encoding='utf-8') as f:\n",
    "            f.write(sentence.serialize())\n",
    "        if not for_cort:\n",
    "            with open(os.path.join(path_plain, header.split('/')[-1] + '_' + part + '.en.txt'), 'a', encoding='utf-8') as f:\n",
    "                f.write(' '.join([t['3'] for t in sentence]) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_conllu = '.\\\\OntoNotes\\\\test.english.v4_gold_conll'\n",
    "path_out = '.\\\\scorer\\\\key_ontonotes'\n",
    "path_plain = '.\\\\OntoNotes\\\\plain'\n",
    "\n",
    "ontonotes_to_scorer(path_conllu, path_out, path_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1900.en.conll\n",
      "wsj_1901.en.conll\n",
      "wsj_1902.en.conll\n",
      "wsj_1903.en.conll\n",
      "wsj_1904.en.conll\n",
      "wsj_1905.en.conll\n",
      "wsj_1906.en.conll\n",
      "wsj_1907.en.conll\n",
      "wsj_1908.en.conll\n",
      "wsj_1909.en.conll\n",
      "wsj_1910.en.conll\n",
      "wsj_1911.en.conll\n",
      "wsj_1912.en.conll\n",
      "wsj_1913.en.conll\n",
      "wsj_1914.en.conll\n",
      "wsj_1915.en.conll\n",
      "wsj_1916.en.conll\n",
      "wsj_1917.en.conll\n",
      "wsj_1918.en.conll\n",
      "wsj_1919.en.conll\n",
      "wsj_1920.en.conll\n",
      "wsj_1921.en.conll\n",
      "wsj_1922.en.conll\n",
      "wsj_1923.en.conll\n",
      "wsj_1924.en.conll\n",
      "wsj_1925.en.conll\n",
      "wsj_1926.en.conll\n",
      "wsj_1927.en.conll\n",
      "wsj_1928.en.conll\n",
      "wsj_1929.en.conll\n",
      "wsj_1930.en.conll\n",
      "wsj_1931.en.conll\n",
      "wsj_1932.en.conll\n",
      "wsj_1933.en.conll\n",
      "wsj_1934.en.conll\n",
      "wsj_1935.en.conll\n",
      "wsj_1936.en.conll\n",
      "wsj_1937.en.conll\n",
      "wsj_1938.en.conll\n",
      "wsj_1939.en.conll\n",
      "wsj_1940.en.conll\n",
      "wsj_1941.en.conll\n",
      "wsj_1942.en.conll\n",
      "wsj_1943.en.conll\n",
      "wsj_1944.en.conll\n",
      "wsj_1945.en.conll\n",
      "wsj_1946.en.conll\n",
      "wsj_1947.en.conll\n",
      "wsj_1948.en.conll\n",
      "wsj_1949.en.conll\n"
     ]
    }
   ],
   "source": [
    "path_conllu = '.\\\\paws\\\\data\\\\conll'\n",
    "path_out = '.\\\\scorer\\\\cort\\\\key'\n",
    "\n",
    "paws_to_scorer(path_conllu, path_out, for_cort=True)\n",
    "\n",
    "path_conllu = '.\\\\OntoNotes\\\\test.english.v4_gold_conll'\n",
    "path_out = '.\\\\scorer\\\\cort\\\\key_ontonotes'\n",
    "path_plain = ''\n",
    "\n",
    "ontonotes_to_scorer(path_conllu, path_out, path_plain, for_cort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
